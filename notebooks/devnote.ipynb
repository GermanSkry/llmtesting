{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import CSVLoader, PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from typing import List, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'C:/Users/skrge/Documents/GitHub/llmtesting/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_files(directory: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Load and return the content of all CSV files in the given directory.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith(\".csv\"):\n",
    "            file_path = os.path.join(directory, file_name)\n",
    "            loader = CSVLoader(file_path)\n",
    "            documents.extend(loader.load())\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf_files(directory: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load and return the content of all PDF files in the given directory as Document objects.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith(\".pdf\"):\n",
    "            file_path = os.path.join(directory, file_name)\n",
    "            loader = PyPDFLoader(file_path)\n",
    "            documents = loader.load()  \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Split documents into chunks using RecursiveCharacterTextSplitter.\n",
    "    \n",
    "    Args:\n",
    "        documents (List[Document]): List of Document objects to be split.\n",
    "        chunk_size (int): Maximum size of each chunk.\n",
    "        chunk_overlap (int): Overlap size between chunks.\n",
    "\n",
    "    Returns:\n",
    "        List[Document]: List of split Document objects.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_docs(documents: List[Document], chunk_size: int = 400, chunk_overlap: int = 40) -> List[Document]:\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    return text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_files(directory: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Upload all supported file types from a given directory, split PDF content into chunks, and return their content.\n",
    "    \"\"\"\n",
    "    supported_loaders = {\n",
    "        \"csv\": load_csv_files,\n",
    "        \"pdf\": load_pdf_files\n",
    "    }\n",
    "    documents = []\n",
    "\n",
    "    for ext, loader_func in supported_loaders.items():\n",
    "        if ext == \"pdf\":\n",
    "            pdf_documents = loader_func(directory)\n",
    "            documents.extend(split_docs(pdf_documents))  # Split PDFs into chunks\n",
    "        else:\n",
    "            documents.extend(loader_func(directory))\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\skrge\\AppData\\Local\\Temp\\ipykernel_47836\\3493368832.py:1: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from langchain_community.embeddings.ollama import OllamaEmbeddings\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings.ollama import OllamaEmbeddings\n",
    "\n",
    "def llama_embeddings(all_docs, model=\"llama3.1\"):\n",
    "    # Initialize the OllamaEmbeddings with the specified model\n",
    "    embedding_model = OllamaEmbeddings(model=model)\n",
    "    \n",
    "    embeddings = []\n",
    "\n",
    "    for chunk in all_docs:\n",
    "        # Get the embedding for the chunk\n",
    "        chunk_embedding = embedding_model.embed_query(chunk)\n",
    "        embeddings.append(chunk_embedding)\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nomic import NomicEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nomic_embeddings(all_docs, model=\"nomic-embed-text-v1.5\"):\n",
    "    # Initialize the OllamaEmbeddings with the specified model\n",
    "    embedding_model = NomicEmbeddings(model=model)\n",
    "    embeddings = []\n",
    "\n",
    "    for chunk in all_docs:\n",
    "        # Get the embedding for the chunk\n",
    "        chunk_embedding = embedding_model.embed_query(chunk)\n",
    "        embeddings.append(chunk_embedding)\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768 [0.072021484, 0.031341553, -0.1940918, -0.05505371, 0.05355835, -0.014434814, -0.032226562, -0.0023593903, -0.019424438, -0.032226562]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "chunks = [\"This is the first chunk.\", \"This is the second chunk.\"]\n",
    "embeddings = nomic_embeddings(chunks)\n",
    "print(len(embeddings[1]), embeddings[1][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4096 [-0.634800910949707, -2.7458271980285645, 2.647017240524292, 1.2148879766464233, -1.6613185405731201, -2.678941011428833, 0.4575340747833252, 1.8895249366760254, 0.12285317480564117, -1.275457501411438]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "chunks = [\"This is the first chunk.\", \"This is the second chunk.\"]\n",
    "embeddings = llama_embeddings(chunks)\n",
    "print(len(embeddings[1]), embeddings[1][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'C:/Users/skrge/Documents/GitHub/llmtesting/data\\\\sample3.pdf', 'page': 0}, page_content='Elder patients are changing the expectations and the specifics of \\nmedical services.  \\nIn addition, people improve their attitude to  lifestyle, they are  \\nmore sensible  to their health and want to have opportunity  to \\nmonitor its condition.  \\nAt the same time, medical services contain a lot of administrative \\nroutine, repetitive work that can be optimized.')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdfs = load_pdf_files(directory)\n",
    "split_documents = split_docs(pdfs)\n",
    "split_documents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = upload_files(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'C:/Users/skrge/Documents/GitHub/llmtesting/data\\\\sample3.pdf', 'page': 1}, page_content='with access. \\nSo Important conditions for AI to deliver its full potential \\nhealthcare will be the integration of different databases across \\norganizations, strong governance to improve  data quality, and \\ngreater confidence from organizations, doctors, and patients and \\nthe ability to manage the related risks.  \\nIn this case Data architects  and data engineers  will have'),\n",
       " Document(metadata={'source': 'C:/Users/skrge/Documents/GitHub/llmtesting/data\\\\sample3.pdf', 'page': 1}, page_content='significant role in defining how to record, store , structure and \\nshare clinical data so that algorithms can be useful for doctors. \\n \\nСлайд 4 \\nThe interaction  between doctors, medical management, data \\nscientists and artificial intelligence specialists is an important \\nfactor for successful implementation of AI in workf lows. \\nAnd human -machine interactions should be well -designed to'),\n",
       " Document(metadata={'source': 'C:/Users/skrge/Documents/GitHub/llmtesting/data\\\\sample3.pdf', 'page': 1}, page_content='accelerate the digitalization process.  \\nThere are three phases implementation of AI:  \\n1. Solutions address routine, repetitive and largely \\nadministrative tasks , optimizing healthcare operations , that \\nshould increase  adoption  of AI. \\n2. Solutions shift from hospital -based to home -based care, such \\nas remote monitoring, AI -powered alerting systems, or virtual'),\n",
       " Document(metadata={'source': 'C:/Users/skrge/Documents/GitHub/llmtesting/data\\\\sample3.pdf', 'page': 1}, page_content='assistants, to increase ownership of their care.  \\n3. AI solutions in clinical practice based on evidence from \\nclinical trials, with increasing focus on improved and scaled \\nclinical decision -support (CDS) tools.  \\nThe introduction of AI in medical services will reorganize the \\nfield and reduce simple labor -intensive tasks. N ew skills and'),\n",
       " Document(metadata={'source': 'C:/Users/skrge/Documents/GitHub/llmtesting/data\\\\sample3.pdf', 'page': 1}, page_content='competencies should be mastered by medical workers, therefore, to \\nwork effectively with AI, medical education should be updated and \\ninterdisciplinary specialists should be added  to workflow . \\nAnd cybersecurity, data protection and encryptio n are important  \\ntopics that ne ed high-classes specialist s, because potential \\nmedical data leaks will cause huge damage to patients and'),\n",
       " Document(metadata={'source': 'C:/Users/skrge/Documents/GitHub/llmtesting/data\\\\sample3.pdf', 'page': 1}, page_content='healthcare systems.  And security issues still needing to be \\nclarified .')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_docs[-6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "370"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_length = len(all_docs[-4].page_content)\n",
    "content_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
