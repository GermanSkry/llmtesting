{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\skrge\\AppData\\Local\\Temp\\ipykernel_33508\\121515349.py:13: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from langchain_community.embeddings.ollama import OllamaEmbeddings\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import faiss\n",
    "import re\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from uuid import uuid4\n",
    "from langchain_community.document_loaders import CSVLoader, PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings.ollama import OllamaEmbeddings\n",
    "from langchain_nomic import NomicEmbeddings\n",
    "from langchain.schema import Document\n",
    "from typing import List, Union, Tuple, Dict\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'C:/Users/skrge/Documents/GitHub/llmtesting/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_files(directory: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Load and return the content of all CSV files in the given directory.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith(\".csv\"):\n",
    "            file_path = os.path.join(directory, file_name)\n",
    "            loader = CSVLoader(file_path)\n",
    "            documents.extend(loader.load())\n",
    "    return documents\n",
    "\n",
    "def load_csv_files(directory: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load and return the content of all CSV files in the given directory.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith(\".csv\"):\n",
    "            file_path = os.path.join(directory, file_name)\n",
    "            loader = CSVLoader(file_path)\n",
    "            documents.extend(loader.load())\n",
    "    return documents\n",
    "\n",
    "def split_docs(documents: List[Document], chunk_size: int = 400, chunk_overlap: int = 40) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Split documents into chunks using RecursiveCharacterTextSplitter.\n",
    "    \n",
    "    Args:\n",
    "        documents (List[Document]): List of Document objects to be split.\n",
    "        chunk_size (int): Maximum size of each chunk.\n",
    "        chunk_overlap (int): Overlap size between chunks.\n",
    "\n",
    "    Returns:\n",
    "        List[Document]: List of split Document objects.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    return text_splitter.split_documents(documents)\n",
    "\n",
    "def remove_garbage_lines(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes lines that contain mostly numbers, standalone letters, or patterns like 'B = B', 'M = M'.\n",
    "    \"\"\"\n",
    "    cleaned_lines = []\n",
    "    \n",
    "    for line in text.split(\"\\n\"):\n",
    "        line = line.strip()\n",
    "        \n",
    "        # Skip lines that are mostly numbers, letters with =, or repeating patterns\n",
    "        if re.match(r'^([\\d\\s]+|[A-Z]\\s*=\\s*[A-Z]\\s*)+$', line):\n",
    "            continue\n",
    "        \n",
    "        # Skip lines with excessive letter-number-symbol sequences (like slurB B B 0 B B)\n",
    "        if re.search(r'(slurB|B\\s*=\\s*B|M\\s*=\\s*M|Y\\s*=\\s*Y|X\\s*=\\s*X|Z\\s*=\\s*Z)', line):\n",
    "            continue\n",
    "        \n",
    "        cleaned_lines.append(line)\n",
    "    \n",
    "    return \"\\n\".join(cleaned_lines)\n",
    "\n",
    "def load_pdf_files(directory: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load and return the content of all PDF files in the given directory.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith(\".pdf\"):\n",
    "            file_path = os.path.join(directory, file_name)\n",
    "            loader = PyPDFLoader(file_path)\n",
    "            pdf_docs = loader.load()\n",
    "            \n",
    "            for doc in pdf_docs:\n",
    "                doc.page_content = remove_garbage_lines(doc.page_content)  # Clean extracted text\n",
    "\n",
    "            documents.extend(pdf_docs)\n",
    "    return documents\n",
    "\n",
    "def upload_files(directory: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Upload all supported file types from a given directory, split PDF content into chunks, and return their content.\n",
    "    \"\"\"\n",
    "    supported_loaders = {\n",
    "        \"csv\": load_csv_files,\n",
    "        \"pdf\": load_pdf_files\n",
    "    }\n",
    "    documents = []\n",
    "\n",
    "    for ext, loader_func in supported_loaders.items():\n",
    "        loaded_documents = loader_func(directory)\n",
    "        if ext == \"pdf\":\n",
    "            documents.extend(split_docs(loaded_documents))  # Split PDFs into chunks\n",
    "        else:\n",
    "            documents.extend(loaded_documents)\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chunk_id(doc: Document, current_page: int = None, current_page_part: int = 0) -> Tuple[str, int, int]:\n",
    "    \"\"\"\n",
    "    Generate a unique ID for a chunk based on the Document's metadata.\n",
    "\n",
    "    Args:\n",
    "        doc (Document): A langchain Document object containing page content and metadata.\n",
    "        current_page (int, optional): Current page number for PDF chunks. Default is None.\n",
    "        current_page_part (int, optional): Current part number for the current page. Default is 0.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str, int, int]: Generated chunk ID, updated current_page, and updated current_page_part.\n",
    "    \"\"\"\n",
    "    current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    metadata = doc.metadata\n",
    "\n",
    "    # Extract only the file name from the full path\n",
    "    file_name = os.path.basename(metadata.get(\"source\", \"unknown\"))\n",
    "\n",
    "    if \"row\" in metadata:  # For CSV files\n",
    "        chunk_id = f\"{file_name}_row{metadata['row']}_{current_time}\"\n",
    "    \n",
    "    elif \"page\" in metadata:  # For PDF files\n",
    "        page = metadata[\"page\"]\n",
    "\n",
    "        # Update page_part if current_page is the same, otherwise reset it\n",
    "        if current_page == page:\n",
    "            current_page_part += 1\n",
    "        else:\n",
    "            current_page = page\n",
    "            current_page_part = 0\n",
    "\n",
    "        chunk_id = f\"{file_name}_page{page}:part{current_page_part}_{current_time}\"\n",
    "    \n",
    "    else:\n",
    "        chunk_id = f\"{file_name}_{current_time}\"  # Fallback for unknown formats\n",
    "\n",
    "    return chunk_id, current_page, current_page_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "current_page = None\n",
    "current_page_part = 0\n",
    "for doc in all_docs:\n",
    "    chunk_id, current_page, current_page_part = generate_chunk_id(doc, current_page, current_page_part)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_documents(all_docs: List[Document]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Process a list of documents and generate chunk IDs with metadata.\n",
    "\n",
    "    Args:\n",
    "        all_docs (List[Document]): List of langchain Document objects.\n",
    "\n",
    "    Returns:\n",
    "        List[Document]: List of processed Document objects with chunk IDs.\n",
    "    \"\"\"\n",
    "    current_page = None\n",
    "    current_page_part = 0\n",
    "    processed_docs = []\n",
    "\n",
    "    for doc in all_docs:\n",
    "        chunk_id, current_page, current_page_part = generate_chunk_id(doc, current_page, current_page_part)\n",
    "        \n",
    "        # Add chunk_id to the document's metadata\n",
    "        new_metadata = doc.metadata.copy()\n",
    "        new_metadata['chunk_id'] = chunk_id\n",
    "        \n",
    "        # Create a new Document with the updated metadata\n",
    "        processed_doc = Document(\n",
    "            metadata=new_metadata,\n",
    "            page_content=doc.page_content\n",
    "        )\n",
    "        \n",
    "        processed_docs.append(processed_doc)\n",
    "\n",
    "    return processed_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "test_directory = 'C:/Users/skrge/Documents/GitHub/llmtesting/data/test/test'\n",
    "pdf_docs = upload_files(test_directory)\n",
    "proc_docs = process_documents(pdf_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def faiss_db(processed_docs):\n",
    "    embeddings = NomicEmbeddings(model=\"nomic-embed-text-v1.5\")\n",
    "\n",
    "    # Create an empty FAISS index with the appropriate embedding dimension\n",
    "    index = faiss.IndexFlatL2(len(embeddings.embed_query(\"hello world\")))\n",
    "\n",
    "    # Initialize FAISS vector store with an in-memory document store\n",
    "    vector_store = FAISS(\n",
    "        embedding_function=embeddings,  # The function to generate embeddings\n",
    "        index=index,  # The FAISS index to store vectors\n",
    "        docstore=InMemoryDocstore(),  # Stores original documents\n",
    "        index_to_docstore_id={}  # Mapping between FAISS index positions and document IDs\n",
    "    )\n",
    "\n",
    "    # Generate unique IDs for each document\n",
    "    uuids = [str(uuid4()) for _ in range(len(processed_docs))]\n",
    "\n",
    "    # Add documents to the vector store with generated UUIDs\n",
    "    vector_store.add_documents(documents=processed_docs, ids=uuids)\n",
    "\n",
    "    # Return the vector store with stored documents\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = faiss_db(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = r\"C:\\Users\\skrge\\Documents\\GitHub\\llmtesting\\output\"\n",
    "\n",
    "def save_faiss_vector_store(vector_store, output_dir):\n",
    "    #Saves the FAISS vector store to the specified directory.\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(output_dir , exist_ok=True)\n",
    "\n",
    "    # Save the FAISS vector store\n",
    "    vector_store.save_local(output_dir )\n",
    "    print(f\"FAISS index saved at: {output_dir }\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index saved at: C:\\Users\\skrge\\Documents\\GitHub\\llmtesting\\output\n"
     ]
    }
   ],
   "source": [
    "save_faiss_vector_store(vector_store, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_faiss_vector_store(output_dir):\n",
    "\n",
    "    #Loads a FAISS vector store from the specified directory.\n",
    "\n",
    "    embeddings = NomicEmbeddings(model=\"nomic-embed-text-v1.5\")\n",
    "\n",
    "    # Check if the FAISS index exists before loading\n",
    "    if not os.path.exists(os.path.join(output_dir, \"index.faiss\")):\n",
    "        raise FileNotFoundError(f\"No FAISS index found at: {output_dir}\")\n",
    "\n",
    "    # Load the FAISS vector store with safe pickle deserialization\n",
    "    vector_store = FAISS.load_local(\n",
    "        output_dir, \n",
    "        embeddings, \n",
    "        allow_dangerous_deserialization=True  # Allows pickle loading\n",
    "    )\n",
    "\n",
    "    print(f\"FAISS index loaded from: {output_dir}\")\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index loaded from: C:\\Users\\skrge\\Documents\\GitHub\\llmtesting\\output\n"
     ]
    }
   ],
   "source": [
    "vector_store = load_faiss_vector_store(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the {question} based only on the following context:\n",
    "\n",
    "{context}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_rag_chat(query_text: str, vector_store):\n",
    "    \"\"\"\n",
    "    Searches the FAISS vector store and generates a response using retrieved documents.\n",
    "    \"\"\"\n",
    "    # Perform similarity search in the existing vector store\n",
    "    results = vector_store.similarity_search(query_text, k=5)\n",
    "\n",
    "    # If no relevant documents are found, return a message\n",
    "    if not results:\n",
    "        print(\"No relevant context found.\")\n",
    "        return None, \"No relevant context found.\", []\n",
    "\n",
    "    # Extract text from retrieved documents\n",
    "    context = \" \".join([doc.page_content for doc in results])\n",
    "\n",
    "    # Format the prompt\n",
    "    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "    prompt = prompt_template.format(context=context, question=query_text)\n",
    "\n",
    "    # Initialize the Ollama model for generating answers\n",
    "    generation_model = OllamaLLM(model=\"llama3.1\")\n",
    "\n",
    "    # Generate response\n",
    "    response = generation_model.invoke(prompt)\n",
    "\n",
    "    # Extract document sources\n",
    "    sources = [doc.metadata.get(\"chunk_id\") for doc in results]\n",
    "    #print(response, sources, context)\n",
    "    return response, context, sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Based on the provided context, George Seyfried wrote \"Disaster at the Arch\", a book about a boy\\'s survival story after the Gateway Arch in St. Louis catches fire and separates him from his family.',\n",
       " ': 7\\nBook: Disaster at the Arch\\nAuthor: George Seyfried\\nDescription: It could have been such a great trip. James Smith is really excited for summer vacation. He is going to St. Louis! After a fun week in the Gateway to the West they finally decide to go to the top of the Arch. There\\'s just one problem. The Arch catches on fire. James gets separated from his family and has to survive on astronaut food and other gift-shop items. Will he make it out of the Arch? Will anyone?George Seyfried was ten when he wrote this book. He lives in Southport, Connecticut with his parents and sister and brother. He loves history, geography, traveling, and goes on cool road trips with his dad every year. He has been to four countries and thirty states and has a goal of visiting all fifty states before he turns 50.\\nAvg_Rating: 3.67\\nNum_Ratings: 3\\nURL: https://www.goodreads.com/book/show/17969389-disaster-at-the-arch : 122\\nBook: The Grifters\\nAuthor: Jim Thompson\\nDescription: To his friends, to his coworkers, and even to his mistress Moira, Roy Dillon is an honest hardworking salesman. He lives in a cheap hotel just within his pay bracket. He goes to work every day. He has hundreds of friends and associates who could attest to his good character.Yet, hidden behind three gaudy clown paintings in Roy\\'s pallid hotel room, sits fifty-two thousand dollars--the money Roy makes from his short cons, his \"grifting.\" For years, Roy has effortlessly maintained control over his house-of-cards life--until the simplest con goes wrong, and he finds himself critically injured and at the mercy of the most dangerous woman he ever met: his own mother.THE GRIFTERS, one of the best novels ever written about the art of the con, is an ingeniously crafted story of deception and betrayal that was the basis for Stephen Frears\\' and Martin Scorsese\\'s 1990 critically-acclaimed film of the same name.\\nAvg_Rating: 3.93\\nNum_Ratings: 11,230\\nURL: https://www.goodreads.com/book/show/19161914-the-grifters : 186\\nBook: The Fifth Column and the First Forty-Nine Stories\\nAuthor: Ernest Hemingway\\nDescription: Hemingway\\'s first forty-nine stories and play.\\nAvg_Rating: 3.84\\nNum_Ratings: 100\\nURL: https://www.goodreads.com/book/show/18137589-the-fifth-column-and-the-first-forty-nine-stories : 125\\nBook: Hard Times\\nAuthor: Charles Dickens\\nDescription: \"My satire is against those who see figures and averages, and nothing else,\" proclaimed Charles Dickens in explaining the theme of this classic novel. Published in 1854, the story concerns one Thomas Gradgrind, a \"fanatic of the demonstrable fact,\" who raises his children, Tom and Louisa, in a stifling and arid atmosphere of grim practicality.Without a moral compass to guide them, the children sink into lives of desperation and despair, played out against the grim background of Coketown, a wretched community shadowed by an industrial behemoth. Louisa falls into a loveless marriage with Josiah Bouderby, a vulgar banker, while the unscrupulous Tom, totally lacking in principle, becomes a thief who frames an innocent man for his crime. Witnessing the degradation and downfall of his children, Gradgrind realizes that his own misguided principles have ruined their lives.Considered Dickens\\' harshest indictment of mid-19th-century industrial practices and their dehumanizing effects, this novel offers a fascinating tapestry of Victorian life, filled with the richness of detail, brilliant characterization, and passionate social concern that typify the novelist\\'s finest creations.Of Dickens\\' work, the eminent Victorian critic John Ruskin had this to say: \"He is entirely right in his main drift and purpose in every book he has written; and all of them, but especially Hard Times, should be studied with close and earnest care by persons interested in social questions.\"\\nAvg_Rating: 3.54\\nNum_Ratings: 65,177\\nURL: https://www.goodreads.com/book/show/5344.Hard_Times : 0\\nBook: Pygmalion\\nAuthor: George Bernard Shaw\\nDescription: One of George Bernard Shaw\\'s best-known plays, Pygmalion was a rousing success on the London and New York stages, an entertaining motion picture and a great hit with its musical version, My Fair Lady. An updated and considerably revised version of the ancient Greek legend of Pygmalion and Galatea, the 20th-century story pokes fun at the antiquated British class system. In Shaw\\'s clever adaptation, Professor Henry Higgins, a linguistic expert, takes on a bet that he can transform an awkward cockney flower seller into a refined young lady simply by polishing her manners and changing the way she speaks. In the process of convincing society that his creation is a mysterious royal figure, the Professor also falls in love with his elegant handiwork.The irresistible theme of the emerging butterfly, together with Shaw\\'s brilliant dialogue and splendid skills as a playwright, have made Pygmalion one of the most popular comedies in the English language. A staple of college drama courses, it is still widely performed.\\nAvg_Rating: 3.89\\nNum_Ratings: 99,531\\nURL: https://www.goodreads.com/book/show/7714.Pygmalion',\n",
       " ['sample_bookcsv.csv_row7_2025-01-16 21:26:20',\n",
       "  'sample_bookcsv.csv_row122_2025-01-16 21:26:20',\n",
       "  'sample_bookcsv.csv_row186_2025-01-16 21:26:20',\n",
       "  'sample_bookcsv.csv_row125_2025-01-16 21:26:20',\n",
       "  'sample_bookcsv.csv_row0_2025-01-16 21:26:20'])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_text = \"What books did write George Seyfried?\"\n",
    "    \n",
    "# Perform the RAG query\n",
    "query_rag_chat(query_text, vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_title_template = \"\"\"\n",
    "    You are an expert in extracting book titles from structured lists.\n",
    "    Please extract only the book titles and return in this format:  \n",
    "\n",
    "    [\"Full_title_1\",\n",
    "    \"Full_title_2\",\n",
    "        ...,\n",
    "    \"Full_title_N\"]\n",
    "\n",
    "    Rules:\n",
    "    - The book title always starts with an upper letter.\n",
    "    - Titles can be placed on few lines in a row.\n",
    "    - Preserve the entire title, including parts before and after `:` if present.\n",
    "    - Preserve the full title, including subtitles and special characters.\n",
    "    - Do NOT add any extra text, explanations, or formatting.\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "def extract_book_title(documents: List[Document]):\n",
    "       \n",
    "    book_info_template = \"\"\"\n",
    "    You are an expert in extracting structured book information from formatted lists.\n",
    "    Please extract the details for each book and return them in this format:\n",
    "\n",
    "    \n",
    "    [\"title_info_1\",\n",
    "    \"title_info_2\",\n",
    "    ...,\n",
    "    \"title_info_n\"]\n",
    "\n",
    "    Rules:\n",
    "    - The book title always starts with an upper letter.\n",
    "    - Titles can be placed on few lines in a row.\n",
    "    - Preserve the entire title, including parts before and after `:` if present.\n",
    "    - Preserve the full title, including subtitles and special characters.\n",
    "    - Do NOT add any extra text, explanations, or formatting.\n",
    "    - Stop extraction before ISBN or any price or sales information ISBN(e.g., Euro 38,80).\n",
    "\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Combine content from all documents\n",
    "    context = \" \".join([doc.page_content for doc in documents])\n",
    "    \n",
    "    # Format the prompt\n",
    "    book_info_template = PromptTemplate(template=book_info_template, input_variables=[\"context\"])\n",
    "    prompt_title = book_info_template.format(context=context)\n",
    "    \n",
    "    # Initialize LLM\n",
    "    generation_model = OllamaLLM(model=\"llama3.1\")\n",
    "    \n",
    "    # Generate response\n",
    "    response = generation_model.invoke(prompt_title)\n",
    " \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_directory = 'C:/Users/skrge/Documents/GitHub/llmtesting/data/test/test'\n",
    "pdf_docs = upload_files(test_directory)\n",
    "proc_docs = process_documents(pdf_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pdf_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def group_documents_by_source_and_page(documents):\n",
    "    \"\"\"\n",
    "    Groups a list of langchain Document objects by their source and page.\n",
    "    Args:\n",
    "        documents (list): A list of langchain Document objects. \n",
    "    Returns:\n",
    "        dict: A dictionary where keys are tuples (source, page), and values are lists of documents.\n",
    "    \"\"\"\n",
    "    documents_grouped = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "    for doc in documents:\n",
    "        source = doc.metadata['source']\n",
    "        page = doc.metadata['page']\n",
    "        \n",
    "        # Group documents by source and page\n",
    "        documents_grouped[source][page].append(doc)\n",
    "\n",
    "    # Flatten into a dictionary of (source, page) -> list of documents\n",
    "    grouped_documents = {}\n",
    "    for source, pages in documents_grouped.items():\n",
    "        for page, docs in pages.items():\n",
    "            grouped_documents[(source, page)] = docs\n",
    "\n",
    "    return grouped_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_titles_from_grouped_documents(documents: List[Document]):\n",
    "    grouped_documents = group_documents_by_source_and_page(documents)\n",
    "    \n",
    "    all_titles = []\n",
    "    \n",
    "    # Apply extract_book_title to each group with progress bar\n",
    "    for (source, page), docs in tqdm(grouped_documents.items(), desc=\"Processing Groups\", unit=\"group\"):\n",
    "        titles = extract_book_title(docs)\n",
    "        all_titles.append(titles)  # Add the titles of the current group to the unified list\n",
    "    \n",
    "    return all_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups:  67%|██████▋   | 2/3 [16:57<08:28, 508.70s/group]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[129], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m titles_info \u001b[38;5;241m=\u001b[39m \u001b[43mextract_titles_from_grouped_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproc_docs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m titles_info\n",
      "Cell \u001b[1;32mIn[36], line 8\u001b[0m, in \u001b[0;36mextract_titles_from_grouped_documents\u001b[1;34m(documents)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Apply extract_book_title to each group with progress bar\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (source, page), docs \u001b[38;5;129;01min\u001b[39;00m tqdm(grouped_documents\u001b[38;5;241m.\u001b[39mitems(), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing Groups\u001b[39m\u001b[38;5;124m\"\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroup\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m----> 8\u001b[0m     titles \u001b[38;5;241m=\u001b[39m \u001b[43mextract_book_title\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     all_titles\u001b[38;5;241m.\u001b[39mappend(titles)  \u001b[38;5;66;03m# Add the titles of the current group to the unified list\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m all_titles\n",
      "Cell \u001b[1;32mIn[128], line 40\u001b[0m, in \u001b[0;36mextract_book_title\u001b[1;34m(documents)\u001b[0m\n\u001b[0;32m     37\u001b[0m generation_model \u001b[38;5;241m=\u001b[39m OllamaLLM(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama3.1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Generate response\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mgeneration_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_title\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\skrge\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\language_models\\llms.py:390\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    382\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    387\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    388\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 390\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    391\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    392\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    393\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    394\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    395\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    396\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    397\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    398\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    399\u001b[0m         )\n\u001b[0;32m    400\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    401\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    402\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\skrge\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\language_models\\llms.py:755\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    749\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    752\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    753\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    754\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 755\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_strings, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\skrge\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\language_models\\llms.py:950\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    935\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    936\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    937\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    938\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialized,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    948\u001b[0m         )\n\u001b[0;32m    949\u001b[0m     ]\n\u001b[1;32m--> 950\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_helper(\n\u001b[0;32m    951\u001b[0m         prompts, stop, run_managers, \u001b[38;5;28mbool\u001b[39m(new_arg_supported), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    952\u001b[0m     )\n\u001b[0;32m    953\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m    954\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\skrge\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\language_models\\llms.py:792\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    790\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[0;32m    791\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 792\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    793\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    794\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[1;32mc:\\Users\\skrge\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\language_models\\llms.py:779\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    769\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    770\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    771\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    775\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    776\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    778\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 779\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    780\u001b[0m                 prompts,\n\u001b[0;32m    781\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    782\u001b[0m                 \u001b[38;5;66;03m# TODO: support multiple run managers\u001b[39;00m\n\u001b[0;32m    783\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    784\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    785\u001b[0m             )\n\u001b[0;32m    786\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    787\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    788\u001b[0m         )\n\u001b[0;32m    789\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    790\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\Users\\skrge\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_ollama\\llms.py:288\u001b[0m, in \u001b[0;36mOllamaLLM._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    286\u001b[0m generations \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[1;32m--> 288\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream_with_aggregation(\n\u001b[0;32m    289\u001b[0m         prompt,\n\u001b[0;32m    290\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    291\u001b[0m         run_manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[0;32m    292\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    293\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    294\u001b[0m     )\n\u001b[0;32m    295\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([final_chunk])\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[1;32mc:\\Users\\skrge\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_ollama\\llms.py:256\u001b[0m, in \u001b[0;36mOllamaLLM._stream_with_aggregation\u001b[1;34m(self, prompt, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_stream_with_aggregation\u001b[39m(\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    249\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    254\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m GenerationChunk:\n\u001b[0;32m    255\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 256\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m stream_resp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_generate_stream(prompt, stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    257\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(stream_resp, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    258\u001b[0m             chunk \u001b[38;5;241m=\u001b[39m GenerationChunk(\n\u001b[0;32m    259\u001b[0m                 text\u001b[38;5;241m=\u001b[39mstream_resp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m stream_resp \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    260\u001b[0m                 generation_info\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    261\u001b[0m                     \u001b[38;5;28mdict\u001b[39m(stream_resp) \u001b[38;5;28;01mif\u001b[39;00m stream_resp\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    262\u001b[0m                 ),\n\u001b[0;32m    263\u001b[0m             )\n",
      "File \u001b[1;32mc:\\Users\\skrge\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_ollama\\llms.py:211\u001b[0m, in \u001b[0;36mOllamaLLM._create_generate_stream\u001b[1;34m(self, prompt, stop, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_generate_stream\u001b[39m(\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    207\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    208\u001b[0m     stop: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    210\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Union[Mapping[\u001b[38;5;28mstr\u001b[39m, Any], \u001b[38;5;28mstr\u001b[39m]]:\n\u001b[1;32m--> 211\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m    212\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_params(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    213\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\skrge\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ollama\\_client.py:169\u001b[0m, in \u001b[0;36mClient._request.<locals>.inner\u001b[1;34m()\u001b[0m\n\u001b[0;32m    166\u001b[0m   e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m    167\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mtext, e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 169\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39miter_lines():\n\u001b[0;32m    170\u001b[0m   part \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(line)\n\u001b[0;32m    171\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;241m:=\u001b[39m part\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\skrge\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\httpx\\_models.py:861\u001b[0m, in \u001b[0;36mResponse.iter_lines\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    859\u001b[0m decoder \u001b[38;5;241m=\u001b[39m LineDecoder()\n\u001b[0;32m    860\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request):\n\u001b[1;32m--> 861\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_text():\n\u001b[0;32m    862\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m decoder\u001b[38;5;241m.\u001b[39mdecode(text):\n\u001b[0;32m    863\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m line\n",
      "File \u001b[1;32mc:\\Users\\skrge\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\httpx\\_models.py:848\u001b[0m, in \u001b[0;36mResponse.iter_text\u001b[1;34m(self, chunk_size)\u001b[0m\n\u001b[0;32m    846\u001b[0m chunker \u001b[38;5;241m=\u001b[39m TextChunker(chunk_size\u001b[38;5;241m=\u001b[39mchunk_size)\n\u001b[0;32m    847\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request):\n\u001b[1;32m--> 848\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m byte_content \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_bytes():\n\u001b[0;32m    849\u001b[0m         text_content \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39mdecode(byte_content)\n\u001b[0;32m    850\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunker\u001b[38;5;241m.\u001b[39mdecode(text_content):\n",
      "File \u001b[1;32mc:\\Users\\skrge\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\httpx\\_models.py:829\u001b[0m, in \u001b[0;36mResponse.iter_bytes\u001b[1;34m(self, chunk_size)\u001b[0m\n\u001b[0;32m    827\u001b[0m chunker \u001b[38;5;241m=\u001b[39m ByteChunker(chunk_size\u001b[38;5;241m=\u001b[39mchunk_size)\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request):\n\u001b[1;32m--> 829\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m raw_bytes \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_raw():\n\u001b[0;32m    830\u001b[0m         decoded \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39mdecode(raw_bytes)\n\u001b[0;32m    831\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunker\u001b[38;5;241m.\u001b[39mdecode(decoded):\n",
      "File \u001b[1;32mc:\\Users\\skrge\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\httpx\\_models.py:883\u001b[0m, in \u001b[0;36mResponse.iter_raw\u001b[1;34m(self, chunk_size)\u001b[0m\n\u001b[0;32m    880\u001b[0m chunker \u001b[38;5;241m=\u001b[39m ByteChunker(chunk_size\u001b[38;5;241m=\u001b[39mchunk_size)\n\u001b[0;32m    882\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request):\n\u001b[1;32m--> 883\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m raw_stream_bytes \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream:\n\u001b[0;32m    884\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_bytes_downloaded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(raw_stream_bytes)\n\u001b[0;32m    885\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunker\u001b[38;5;241m.\u001b[39mdecode(raw_stream_bytes):\n",
      "File \u001b[1;32mc:\\Users\\skrge\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\httpx\\_client.py:126\u001b[0m, in \u001b[0;36mBoundSyncStream.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mIterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[1;32m--> 126\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream:\n\u001b[0;32m    127\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n",
      "File \u001b[1;32mc:\\Users\\skrge\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\httpx\\_transports\\default.py:113\u001b[0m, in \u001b[0;36mResponseStream.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mIterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[1;32m--> 113\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_httpcore_stream:\n\u001b[0;32m    114\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m part\n",
      "File \u001b[1;32mc:\\Users\\skrge\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\httpcore\\_sync\\connection_pool.py:367\u001b[0m, in \u001b[0;36mPoolByteStream.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    366\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m--> 367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\skrge\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\httpcore\\_sync\\connection_pool.py:363\u001b[0m, in \u001b[0;36mPoolByteStream.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 363\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream:\n\u001b[0;32m    364\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m part\n\u001b[0;32m    365\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Users\\skrge\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\httpcore\\_sync\\http11.py:349\u001b[0m, in \u001b[0;36mHTTP11ConnectionByteStream.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ShieldCancellation():\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m--> 349\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[1;32mc:\\Users\\skrge\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\httpcore\\_sync\\http11.py:341\u001b[0m, in \u001b[0;36mHTTP11ConnectionByteStream.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    340\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_body\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request, kwargs):\n\u001b[1;32m--> 341\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39m_receive_response_body(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    342\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;66;03m# If we get an exception while streaming the response,\u001b[39;00m\n\u001b[0;32m    345\u001b[0m     \u001b[38;5;66;03m# we want to close the response (and possibly the connection)\u001b[39;00m\n\u001b[0;32m    346\u001b[0m     \u001b[38;5;66;03m# before raising that exception.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\skrge\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\httpcore\\_sync\\http11.py:210\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_body\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    207\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 210\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mData):\n\u001b[0;32m    212\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mbytes\u001b[39m(event\u001b[38;5;241m.\u001b[39mdata)\n",
      "File \u001b[1;32mc:\\Users\\skrge\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\httpcore\\_sync\\http11.py:224\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    221\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[1;32m--> 224\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[0;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[1;32mc:\\Users\\skrge\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\httpcore\\_backends\\sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[1;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[1;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "titles_info = extract_titles_from_grouped_documents(proc_docs)\n",
    "titles_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[\"A Magazine\", \\n\"MAK: The Architecture of Byoungsoo Cho*\", \\n\"Archives 7: Francisco Mangado\", \\n\"Archives 6: Solano Benítez & Gloria Cabral\", \\n\"Encounters with Plečnik\", \\n\"Pitsou Kedem Architects – Works and Projects\", \\n\"Robin Boyd: Late Works\", \\n\"Dudok by Iwan Baan\"]',\n",
       " '[\"Immortal: Lost Memoirs of Cornelia Dulac Concerning the Freshwater Polyp Hydra\",\\n    \"The Wanderer*\",\\n    \"Bud Book\",\\n    \"Clouds and Bombs*\",\\n    \"Jörg Schmeisser Retrospective: Neverending Journeys\",\\n    \"Paradise On Paper Where Flowers Bloom, Birds Sing\",\\n    \"Mirror Creation*\",\\n    \"Practice of Spiral Practice of Spiral\"]',\n",
       " '[\"Goblins\", \"The Cult of Water\", \"Satan is Real: Two Short Stories\", \"Empty Aphrodite: An Encyclopaedia of Fate\", \"Bruce Hamana Sosei – 100 Beautiful Words in the Way of Tea\", \"Aesthetics as Space\", \"Errant Journal 1: Where are We?\", \"Unpacking My Library\"]']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_isbn_numbers(isbn_strings: List[str]) -> List[int]:\n",
    "    isbn_numbers = []\n",
    "    \n",
    "    for isbn_str in isbn_strings:\n",
    "        # Find all ISBN numbers in the string\n",
    "        matches = re.findall(r'\\d{13}', isbn_str)\n",
    "        # Convert matches to integers and add to the list\n",
    "        isbn_numbers.extend([int(match) for match in matches])\n",
    "    \n",
    "    return isbn_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List\n",
    "\n",
    "def combine_text_info(json_strings: List[str]) -> List[str]:\n",
    "    combined_list = []\n",
    "    \n",
    "    for json_str in json_strings:\n",
    "        try:\n",
    "            # Parse the JSON string\n",
    "            text_list = json.loads(json_str)\n",
    "            # Extend the combined list with the parsed list\n",
    "            combined_list.extend(text_list)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error decoding JSON: {json_str}\")\n",
    "            continue\n",
    "    \n",
    "    return combined_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error decoding JSON: Here are the extracted book details:\n",
      "\n",
      "[\"A Magazine, Antwerp 2020\", \"Architectural Publisher B, Copenhagen 2020\", \"C2C Editorial, La Coruña 2020\", \"C2C Editorial, La Coruña 2020\", \"Museum of Architecture and Design, Ljubljana 2020\", \"A.MAG Editora, Porto 2020\", \"URO Publications, Melbourne 2020\", \"nai010 Publishers, Rotterdam 2020\"]\n",
      "Error decoding JSON: Here are the extracted book details in the requested format:\n",
      "\n",
      "\n",
      "[\"4380. Immortal: Lost Memoirs of Cornelia Dulac Concerning the Freshwater Polyp Hydra\",\n",
      "\"Aalto University, Helsinki 2020\",\n",
      "\"Canadian biologist Cornelia Dulac has been missing since 2014.\",\n",
      "\"Her audiotapes were discovered at a remote cabin in Eastern Finland, itself a fully-equipped research laboratory with a freshwater research laboratory with a freshwater well, gasoline for a generator, and a year’s supply of food.\",\n",
      "\"She had been researching hydra, a seemingly immortal freshwater polyp. Obviously, something interrupted Dulac’s plans.\",\n",
      "\"This book by artist Maija Tammi and illustrator and graphic designer Ville Tietäväinen combines scientific research, art, and storytelling in a complex visualisation of the regenerative hydra while delving into Dulac’s obsession.\",\n",
      "\"79. Monique Besten – The Wanderer*\",\n",
      "\"dpr-barcelona, Barcelona 2020\",\n",
      "\"Monique Besten is an artist and writer researching notions of home by moving through the world in search of narra - tives of all scales, in the process making connections between the past and future, between people, and between life and death.\",\n",
      "\"She invites us to wander, both physically and mentally, in a quest for meaning and belonging.\",\n",
      "\"This book is part of her journey and offers insights into her creative process and artistic vision.\",\n",
      "\"77. Tsuyoshi Hisakado – Practice of Spiral \",\n",
      "\"Torch Press, Tokyo 2020\",\n",
      "\"Appearing on the occasion of Tsuyoshi Hisakado’s solo exhibition, this publication documents the newly commissioned installations the artist conceived for the Tokyo Municipal Museum of Art, which respond to the museum’s interior.\",\n",
      "\"Hisakado collects everyday phenomena, the memories of particular locations and history, harnessing these elements as fragments of sound, light, and sculptural forms in order to create spaces that quietly yet powerfully affect the physical senses of the viewer.\",\n",
      "\"The work encourages us to refine our perceptions and poses meta phorical questions about concepts of eternity.\"]\n",
      "\n",
      "Note: I have reformatted the text to match the requested format. Let me know if you'd like me to modify anything!\n",
      "Error decoding JSON: Here are the extracted details for each book in the required format:\n",
      "\n",
      "[\"Goblins Rough Trade Books, London 2020\", \"The Cult of Water Rough Trade Books, London 2020\", \"Satan is Real: Two Short Stories Rough Trade Books, London 2020\", \"Empty Aphrodite: An Encyclopaedia of Fate Rough Trade Books, London 2020\", \"Bruce Hamana Sosei – 100 Beautiful Words in the Way of Tea Tankosha, Kyoto 2020\", \"Ossi Naukkarinen – Aesthetics as Space Aalto University, Helsinki 2020\", \"Errant Journal 1: Where are We? Errant Journal, Amsterdam 2020\", \"Allen S. Weiss – Unpacking My Library K. Verlag, Berlin 2020\"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_name = combine_text_info(titles_info)\n",
    "book_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "    Based on the provided document, extract the following details and return ONLY in valid JSON format:\n",
    "\n",
    "    {{\n",
    "        \"Full_title\": \"<Name or Title>\",\n",
    "        \"Publisher_title\": \"<Publisher Title>\",\n",
    "        \"City\": \"<City Name>\",\n",
    "        \"Year\": \"<Year>\",\n",
    "        \"ISBN\": \"<Only ISBN number>\",\n",
    "        \"price\": \"<Only numeric price>\",\n",
    "        \"book_shop_name\": \"<Bookshop Name>\",\n",
    "        \"book_shop_id\": \"<Only numeric Bookshop ID>\",\n",
    "        \"pages\": \"<Only number of pages>\",\n",
    "        \"colour\": \"<Colour details>\",\n",
    "        \"size\": \"<Size>\",\n",
    "        \"language\": \"<Language>\"\n",
    "    }}\n",
    "\n",
    "    If multiple books are found, return them as separate JSON objects.\n",
    "    Do NOT add any extra text, explanations, or formatting.\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import VectorStore\n",
    "\n",
    "def extract_book_info(documents: List[Document], book_list: List[str], vector_store: VectorStore):\n",
    "    PROMPT_TEMPLATE = \"\"\"Based on the provided document, extract the following details and return ONLY in valid JSON format:\n",
    "    {{\n",
    "        \"Full_title\": \"<Full_title>\",\n",
    "        \"City\": \"<City Name>\",\n",
    "        \"Year\": \"<Year>\",\n",
    "        \"ISBN\": \"<ISBN number>\",\n",
    "        \"price\": \"numeric price>\",\n",
    "        \"book_shop_id\": \"<Only numeric Bookshop ID>\",\n",
    "        \"pages\": \"<Only number of pages>\",\n",
    "        \"colour\": \"<Colour details>\",\n",
    "        \"size\": \"<Size>\",\n",
    "        \"language\": \"<Language>\"\n",
    "    }}\n",
    "\n",
    "    Do NOT add any extra text, explanations, or formatting.\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "    \"\"\"\n",
    "\n",
    "    extracted_info = []\n",
    "    generation_model = OllamaLLM(model=\"llama3.1\")  # Initialize LLM once\n",
    "\n",
    "    for book in tqdm(book_list, desc=\"Processing books\", unit=\"book\"):\n",
    "        query = f\"Find details about '{book}'\"\n",
    "        results = vector_store.similarity_search(query, k=5)\n",
    "\n",
    "        # Filter results to ensure they are related to the current book\n",
    "        #filtered_results = [doc for doc in results if book.lower() in doc.page_content.lower()]\n",
    "        context = \" \".join([result.page_content for result in results])\n",
    "\n",
    "        # If no relevant documents are found, skip processing\n",
    "        if not context.strip():\n",
    "            print(f\"Warning: No relevant context found for book '{book}'\")\n",
    "            extracted_info.append(f'{{\"Full_title\": \"{book}\", \"error\": \"No data found\"}}')\n",
    "            continue\n",
    "\n",
    "        # Generate response\n",
    "        prompt = PROMPT_TEMPLATE.format(context=context)\n",
    "        response = generation_model.invoke(prompt).strip()\n",
    "\n",
    "        extracted_info.append(response)\n",
    "\n",
    "    return extracted_info  # Return list of extracted book details in string format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_store = faiss_db(proc_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing books: 100%|██████████| 24/24 [15:49<00:00, 39.58s/book]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['{\\n    \"Full_title\": \"A Magazine, Antwerp 2020\",\\n    \"City\": \"Antwerp\",\\n    \"Year\": \"2020\",\\n    \"ISBN\": \"9789077745212\",\\n    \"price\": \"15.50\",\\n    \"book_shop_id\": \"20253\",\\n    \"pages\": \"222\",\\n    \"colour\": \"colour & bw\",\\n    \"size\": \"17 x 21 cm\",\\n    \"language\": \"English\"\\n}', '{\\n    \"Full_title\": \"The Architecture of Byoungsoo Cho\",\\n    \"City\": \"Copenhagen\",\\n    \"Year\": \"2020\",\\n    \"ISBN\": \"9788792700322\",\\n    \"price\": \"61.70\",\\n    \"book_shop_id\": \"\",\\n    \"pages\": \"408\",\\n    \"colour\": \"colour & bw\",\\n    \"size\": \"23 x 33 cm\",\\n    \"language\": \"English\"\\n}', '{\\n    \"Full_title\": \"Practice of Spiral Torch Press, Tokyo 2020\",\\n    \"City\": \"Tokyo\",\\n    \"Year\": \"2020\",\\n    \"ISBN\": \"9784907562212\",\\n    \"price\": \"49.50\",\\n    \"book_shop_id\": \"20247\",\\n    \"pages\": \"304\",\\n    \"colour\": \"colour & bw\",\\n    \"size\": \"17 x 24 cm\",\\n    \"language\": \"Spanish/English\"\\n}', '{\\n    \"Full_title\": \"16. Encounters with Plečnik\",\\n    \"City\": \"La Coruña\",\\n    \"Year\": \"2020\",\\n    \"ISBN\": \"9788412162516\",\\n    \"price\": \"25.20\",\\n    \"book_shop_id\": \"20203\",\\n    \"pages\": \"304\",\\n    \"colour\": \"ills colour & bw\",\\n    \"size\": \"17 x 24 cm\",\\n    \"language\": \"Spanish/English\"\\n}', '{\\n  \"Full_title\": \"52 p, ills colour & bw, 15 x 21 cm, pb, Slovenian/English\",\\n  \"City\": \"Porto\",\\n  \"Year\": \"2020\",\\n  \"ISBN\": \"9789895462049\",\\n  \"price\": \"49.50\",\\n  \"book_shop_id\": \"20161\",\\n  \"pages\": \"52\",\\n  \"colour\": \"ills colour & bw\",\\n  \"size\": \"15 x 21 cm\",\\n  \"language\": \"Slovenian/English\"\\n}', '{\\n    \"Full_title\": \"Pitsou Kedem Architects – Works and Projects\",\\n    \"City\": \"\",\\n    \"Year\": \"2020\",\\n    \"ISBN\": \"9789895462049\",\\n    \"price\": \"49.50\",\\n    \"book_shop_id\": \"20161\",\\n    \"pages\": \"52\",\\n    \"colour\": \"ills colour & bw\",\\n    \"size\": \"15 x 21 cm\",\\n    \"language\": \"Slovenian/English\"\\n}', '{\\n    \"Full_title\": \"Robin Boyd: Late Works\",\\n    \"City\": \"Melbourne\",\\n    \"Year\": \"2020\",\\n    \"ISBN\": \"9780648435594\",\\n    \"price\": \"38.80\",\\n    \"book_shop_id\": \"14\",\\n    \"pages\": \"152\",\\n    \"colour\": \"colour & bw\",\\n    \"size\": \"24 x 28 cm\",\\n    \"language\": \"English\"\\n}', '{\\n    \"Full_title\": \"108 p, ills colour & bw, 22 x 30 cm, pb\",\\n    \"City\": \"\",\\n    \"Year\": \"2020\",\\n    \"ISBN\": \"9789462085817\",\\n    \"price\": \"39.95\",\\n    \"book_shop_id\": \"\",\\n    \"pages\": \"108\",\\n    \"colour\": \"ills colour & bw\",\\n    \"size\": \"22 x 30 cm\",\\n    \"language\": \"English\"\\n}', '{\\n    \"Full_title\": \"4380. Immortal: Lost Memoirs of Cornelia Dulac Concerning the Freshwater Polyp Hydra\",\\n    \"City\": \"Helsinki\",\\n    \"Year\": \"2020\",\\n    \"ISBN\": \"9789526089621\",\\n    \"price\": \"41.50\",\\n    \"book_shop_id\": \"20116\",\\n    \"pages\": \"128\",\\n    \"colour\": \"ills colour & bw\",\\n    \"size\": \"20 x 22 cm\",\\n    \"language\": \"English\"\\n}', '{\\n    \"Full_title\": \"Monique Besten – The Wanderer*\",\\n    \"City\": \"Barcelona\",\\n    \"Year\": \"2020\",\\n    \"ISBN\": \"9788412039092\",\\n    \"price\": \"15.75\",\\n    \"book_shop_id\": \"dpr-barcelona\",\\n    \"pages\": \"128\",\\n    \"colour\": \"ills colour & bw\",\\n    \"size\": \"20 x 22 cm\",\\n    \"language\": \"English\"\\n}', '{\\n  \"Full_title\": \"Bud Book\",\\n  \"City\": \"Helsinki\",\\n  \"Year\": \"2020\",\\n  \"ISBN\": \"9789525939262\",\\n  \"price\": \"23.20\",\\n  \"book_shop_id\": \"20224\",\\n  \"pages\": \"146\",\\n  \"colour\": \"colour & bw\",\\n  \"size\": \"13 x 20 cm\",\\n  \"language\": \"English\"\\n}', '{\\n    \"Full_title\": \"Juan Hein – Clouds and Bombs*\",\\n    \"City\": \"Copenhagen\",\\n    \"Year\": \"2020\",\\n    \"ISBN\": \"9788797052068\",\\n    \"price\": \"30.85\",\\n    \"book_shop_id\": \"\",\\n    \"pages\": \"182\",\\n    \"colour\": \"ills colour & bw\",\\n    \"size\": \"15 x 21 cm\",\\n    \"language\": \"English\"\\n}', '{\\n    \"Full_title\": \"Art84. Jörg Schmeisser Retrospective: Neverending Journeys\",\\n    \"City\": \"Tokyo\",\\n    \"Year\": \"2020\",\\n    \"ISBN\": \"9784763018267\",\\n    \"price\": \"36,25\",\\n    \"book_shop_id\": \"20158\",\\n    \"pages\": \"44\",\\n    \"colour\": \"colour & bw\",\\n    \"size\": \"20 x 25 cm\",\\n    \"language\": \"English\"\\n}', '{\\n    \"Full_title\": \"Ohara Koson: Paradise On Paper Where Flowers Bloom, Birds Sing\",\\n    \"City\": \"Tokyo\",\\n    \"Year\": \"2019\",\\n    \"ISBN\": \"9784808711290\",\\n    \"price\": \"32.35\",\\n    \"book_shop_id\": \"20313\",\\n    \"pages\": \"172\",\\n    \"colour\": \"ills colour\",\\n    \"size\": \"19 x 26 cm\",\\n    \"language\": \"Japanese\"\\n}', '{\\n    \"Full_title\": \"butterflies.\",\\n    \"City\": \"He He, Tokyo\",\\n    \"Year\": \"2020\",\\n    \"ISBN\": \"9784908062315\",\\n    \"price\": \"83.90\",\\n    \"book_shop_id\": \"\",\\n    \"pages\": \"144\",\\n    \"colour\": \"ills colour\",\\n    \"size\": \"18 x 26 cm\",\\n    \"language\": \"Japanese/English\"\\n}', '{\\n    \"Full_title\": \"Practice of Spiral\",\\n    \"City\": \"Tokyo\",\\n    \"Year\": \"2020\",\\n    \"ISBN\": \"4907562212\",\\n    \"price\": \"49.50\",\\n    \"book_shop_id\": \"20247\",\\n    \"pages\": \"200\",\\n    \"colour\": \"ills colour\",\\n    \"size\": \"23 x 30 cm\",\\n    \"language\": \"Japanese/English\"\\n}', '{\\n    \"Full_title\": \"79156. Goblins\",\\n    \"City\": \"London\",\\n    \"Year\": \"2020\",\\n    \"ISBN\": \"9781912722730\",\\n    \"price\": \"10,50\",\\n    \"book_shop_id\": \"20279\",\\n    \"pages\": \"40\",\\n    \"colour\": \"ills colour & bw\",\\n    \"size\": \"15 x 21 cm\",\\n    \"language\": \"English\"\\n}', '{\\n    \"Full_title\": \"The Cult of Water\",\\n    \"City\": \"London\",\\n    \"Year\": \"2020\",\\n    \"ISBN\": \"9781912722723\",\\n    \"price\": 10.50,\\n    \"book_shop_id\": 20278,\\n    \"pages\": null,\\n    \"colour\": \"colour & bw\",\\n    \"size\": \"15 x 21 cm\",\\n    \"language\": \"English155\"\\n}', '{\\n    \"Full_title\": \"Satan is Real: Two Short Stories\",\\n    \"City\": \"London\",\\n    \"Year\": \"2020\",\\n    \"ISBN\": \"9781912722716\",\\n    \"price\": \"10.50\",\\n    \"book_shop_id\": \"20280\",\\n    \"pages\": \"40\",\\n    \"colour\": \"Colour & bw\",\\n    \"size\": \"15 x 21 cm\",\\n    \"language\": \"English\"\\n}', '{\\n    \"Full_title\": \"Empty Aphrodite: An Encyclopaedia of Fate\",\\n    \"City\": \"Kyoto\",\\n    \"Year\": \"2020\",\\n    \"ISBN\": \"9784473044006\",\\n    \"price\": \"21.10\",\\n    \"book_shop_id\": \"100\",\\n    \"pages\": \"112\",\\n    \"colour\": \"colour & bw\",\\n    \"size\": \"13 x 19 cm\",\\n    \"language\": \"Japanese/English\"\\n}', '{\\n    \"Full_title\": \"100 Beautiful Words in the Way of Tea\",\\n    \"City\": \"Kyoto\",\\n    \"Year\": \"2020\",\\n    \"ISBN\": \"9784473044006\",\\n    \"price\": \"21.10\",\\n    \"book_shop_id\": \"152\",\\n    \"pages\": \"200\",\\n    \"colour\": \"ills colour\",\\n    \"size\": \"23 x 30 cm\",\\n    \"language\": \"Japanese/English\"\\n}', '{\\n    \"Full_title\": \"Errant Journal 1: Where are We?\",\\n    \"City\": \"Amsterdam\",\\n    \"Year\": \"2020\",\\n    \"ISBN\": \"Not provided\",\\n    \"price\": \"38.55\",\\n    \"book_shop_id\": \"20207\",\\n    \"pages\": \"222\",\\n    \"colour\": \"ills colour & bw\",\\n    \"size\": \"17 x 21 cm\",\\n    \"language\": \"English\"\\n}', '{\\n    \"Full_title\": \"Errant Journal, Amsterdam 2020\",\\n    \"City\": \"Amsterdam\",\\n    \"Year\": \"2020\",\\n    \"ISBN\": \"9789083079301\",\\n    \"price\": \"15.00\",\\n    \"book_shop_id\": \"20307\",\\n    \"pages\": \"222\",\\n    \"colour\": \"ills colour & bw\",\\n    \"size\": \"17 x 21 cm\",\\n    \"language\": \"English\"\\n}', '{\\n    \"Full_title\": \"My Library\",\\n    \"City\": \"Berlin\",\\n    \"Year\": \"2020\",\\n    \"ISBN\": \"9783947858156\",\\n    \"price\": \"16.60\",\\n    \"book_shop_id\": \"20306\",\\n    \"pages\": \"160\",\\n    \"colour\": \"no ills\",\\n    \"size\": \"13 x 20 cm\",\\n    \"language\": \"English\"\\n}']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "extracted_info = extract_book_info(proc_docs, book_name, book_store)\n",
    "print(extracted_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from typing import List, Union\n",
    "\n",
    "def preprocess_field(value: Union[str, None], field_type: str) -> str:\n",
    "    if not isinstance(value, str):\n",
    "        return \"\"\n",
    "    if field_type == \"price\":\n",
    "        # Remove any non-numeric characters except for the decimal separator\n",
    "        value = re.sub(r'[^\\d,\\.]', '', value)\n",
    "        # Replace comma with dot if necessary\n",
    "        value = value.replace(',', '.')\n",
    "    elif field_type in [\"ISBN\", \"Year\", \"book_shop_id\", \"pages\"]:\n",
    "        # Remove any non-numeric characters\n",
    "        value = re.sub(r'[^\\d]', '', value)\n",
    "    return value\n",
    "\n",
    "def clean_json_string(json_str: str) -> List[str]:\n",
    "    # Remove leading/trailing non-JSON characters and split into individual JSON objects\n",
    "    json_str = json_str.strip('```').strip()\n",
    "    json_objects = re.findall(r'\\{.*?\\}', json_str, re.DOTALL)\n",
    "    return json_objects\n",
    "\n",
    "def create_dataframe_from_json_strings(json_strings: List[str]) -> pd.DataFrame:\n",
    "    # List to store parsed JSON objects\n",
    "    parsed_data = []\n",
    "    \n",
    "    # Parse each JSON string\n",
    "    for json_str in json_strings:\n",
    "        json_objects = clean_json_string(json_str)\n",
    "        for obj_str in json_objects:\n",
    "            try:\n",
    "                book_info = json.loads(obj_str)\n",
    "                # Preprocess relevant fields\n",
    "                for field in [\"price\", \"ISBN\", \"Year\", \"book_shop_id\", \"pages\"]:\n",
    "                    if field in book_info:\n",
    "                        book_info[field] = preprocess_field(book_info[field], field)\n",
    "                parsed_data.append(book_info)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Error decoding JSON: {obj_str}\")\n",
    "                continue\n",
    "    \n",
    "    # Create DataFrame from parsed data\n",
    "    df = pd.DataFrame(parsed_data)\n",
    "    \n",
    "    # Ensure the DataFrame has the desired columns\n",
    "    desired_columns = [\"Full_title\", \"City\", \"Year\", \"ISBN\", \"price\", \"book_shop_id\", \"pages\", \"colour\", \"size\", \"language\"]\n",
    "    df = df.reindex(columns=desired_columns)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = create_dataframe_from_json_strings(extracted_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Full_title</th>\n",
       "      <th>City</th>\n",
       "      <th>Year</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>price</th>\n",
       "      <th>book_shop_id</th>\n",
       "      <th>pages</th>\n",
       "      <th>colour</th>\n",
       "      <th>size</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Floragatan 13: Curated by Acne Studios</td>\n",
       "      <td>Antwerp</td>\n",
       "      <td>2020</td>\n",
       "      <td>9789077745212</td>\n",
       "      <td>15.50</td>\n",
       "      <td>20253</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MAK: The Architecture of Byoungsoo Cho*</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Full_title     City  Year           ISBN  \\\n",
       "0   Floragatan 13: Curated by Acne Studios  Antwerp  2020  9789077745212   \n",
       "1  MAK: The Architecture of Byoungsoo Cho*      NaN   NaN            NaN   \n",
       "\n",
       "   price book_shop_id pages colour size language  \n",
       "0  15.50        20253                             \n",
       "1    NaN          NaN   NaN    NaN  NaN      NaN  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Full_title</th>\n",
       "      <th>City</th>\n",
       "      <th>Year</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>price</th>\n",
       "      <th>book_shop_id</th>\n",
       "      <th>pages</th>\n",
       "      <th>colour</th>\n",
       "      <th>size</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Magazine, Antwerp 2020</td>\n",
       "      <td>Antwerp</td>\n",
       "      <td>2020</td>\n",
       "      <td>9789077745212</td>\n",
       "      <td>15.50</td>\n",
       "      <td>20253</td>\n",
       "      <td>222</td>\n",
       "      <td>colour &amp; bw</td>\n",
       "      <td>17 x 21 cm</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Architecture of Byoungsoo Cho</td>\n",
       "      <td>Copenhagen</td>\n",
       "      <td>2020</td>\n",
       "      <td>9788792700322</td>\n",
       "      <td>61.70</td>\n",
       "      <td></td>\n",
       "      <td>408</td>\n",
       "      <td>colour &amp; bw</td>\n",
       "      <td>23 x 33 cm</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Practice of Spiral Torch Press, Tokyo 2020</td>\n",
       "      <td>Tokyo</td>\n",
       "      <td>2020</td>\n",
       "      <td>9784907562212</td>\n",
       "      <td>49.50</td>\n",
       "      <td>20247</td>\n",
       "      <td>304</td>\n",
       "      <td>colour &amp; bw</td>\n",
       "      <td>17 x 24 cm</td>\n",
       "      <td>Spanish/English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16. Encounters with Plečnik</td>\n",
       "      <td>La Coruña</td>\n",
       "      <td>2020</td>\n",
       "      <td>9788412162516</td>\n",
       "      <td>25.20</td>\n",
       "      <td>20203</td>\n",
       "      <td>304</td>\n",
       "      <td>ills colour &amp; bw</td>\n",
       "      <td>17 x 24 cm</td>\n",
       "      <td>Spanish/English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52 p, ills colour &amp; bw, 15 x 21 cm, pb, Sloven...</td>\n",
       "      <td>Porto</td>\n",
       "      <td>2020</td>\n",
       "      <td>9789895462049</td>\n",
       "      <td>49.50</td>\n",
       "      <td>20161</td>\n",
       "      <td>52</td>\n",
       "      <td>ills colour &amp; bw</td>\n",
       "      <td>15 x 21 cm</td>\n",
       "      <td>Slovenian/English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Pitsou Kedem Architects – Works and Projects</td>\n",
       "      <td></td>\n",
       "      <td>2020</td>\n",
       "      <td>9789895462049</td>\n",
       "      <td>49.50</td>\n",
       "      <td>20161</td>\n",
       "      <td>52</td>\n",
       "      <td>ills colour &amp; bw</td>\n",
       "      <td>15 x 21 cm</td>\n",
       "      <td>Slovenian/English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Robin Boyd: Late Works</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>2020</td>\n",
       "      <td>9780648435594</td>\n",
       "      <td>38.80</td>\n",
       "      <td>14</td>\n",
       "      <td>152</td>\n",
       "      <td>colour &amp; bw</td>\n",
       "      <td>24 x 28 cm</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>108 p, ills colour &amp; bw, 22 x 30 cm, pb</td>\n",
       "      <td></td>\n",
       "      <td>2020</td>\n",
       "      <td>9789462085817</td>\n",
       "      <td>39.95</td>\n",
       "      <td></td>\n",
       "      <td>108</td>\n",
       "      <td>ills colour &amp; bw</td>\n",
       "      <td>22 x 30 cm</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4380. Immortal: Lost Memoirs of Cornelia Dulac...</td>\n",
       "      <td>Helsinki</td>\n",
       "      <td>2020</td>\n",
       "      <td>9789526089621</td>\n",
       "      <td>41.50</td>\n",
       "      <td>20116</td>\n",
       "      <td>128</td>\n",
       "      <td>ills colour &amp; bw</td>\n",
       "      <td>20 x 22 cm</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Monique Besten – The Wanderer*</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>2020</td>\n",
       "      <td>9788412039092</td>\n",
       "      <td>15.75</td>\n",
       "      <td></td>\n",
       "      <td>128</td>\n",
       "      <td>ills colour &amp; bw</td>\n",
       "      <td>20 x 22 cm</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Bud Book</td>\n",
       "      <td>Helsinki</td>\n",
       "      <td>2020</td>\n",
       "      <td>9789525939262</td>\n",
       "      <td>23.20</td>\n",
       "      <td>20224</td>\n",
       "      <td>146</td>\n",
       "      <td>colour &amp; bw</td>\n",
       "      <td>13 x 20 cm</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Juan Hein – Clouds and Bombs*</td>\n",
       "      <td>Copenhagen</td>\n",
       "      <td>2020</td>\n",
       "      <td>9788797052068</td>\n",
       "      <td>30.85</td>\n",
       "      <td></td>\n",
       "      <td>182</td>\n",
       "      <td>ills colour &amp; bw</td>\n",
       "      <td>15 x 21 cm</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Art84. Jörg Schmeisser Retrospective: Neverend...</td>\n",
       "      <td>Tokyo</td>\n",
       "      <td>2020</td>\n",
       "      <td>9784763018267</td>\n",
       "      <td>36.25</td>\n",
       "      <td>20158</td>\n",
       "      <td>44</td>\n",
       "      <td>colour &amp; bw</td>\n",
       "      <td>20 x 25 cm</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Ohara Koson: Paradise On Paper Where Flowers B...</td>\n",
       "      <td>Tokyo</td>\n",
       "      <td>2019</td>\n",
       "      <td>9784808711290</td>\n",
       "      <td>32.35</td>\n",
       "      <td>20313</td>\n",
       "      <td>172</td>\n",
       "      <td>ills colour</td>\n",
       "      <td>19 x 26 cm</td>\n",
       "      <td>Japanese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>butterflies.</td>\n",
       "      <td>He He, Tokyo</td>\n",
       "      <td>2020</td>\n",
       "      <td>9784908062315</td>\n",
       "      <td>83.90</td>\n",
       "      <td></td>\n",
       "      <td>144</td>\n",
       "      <td>ills colour</td>\n",
       "      <td>18 x 26 cm</td>\n",
       "      <td>Japanese/English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Practice of Spiral</td>\n",
       "      <td>Tokyo</td>\n",
       "      <td>2020</td>\n",
       "      <td>4907562212</td>\n",
       "      <td>49.50</td>\n",
       "      <td>20247</td>\n",
       "      <td>200</td>\n",
       "      <td>ills colour</td>\n",
       "      <td>23 x 30 cm</td>\n",
       "      <td>Japanese/English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>79156. Goblins</td>\n",
       "      <td>London</td>\n",
       "      <td>2020</td>\n",
       "      <td>9781912722730</td>\n",
       "      <td>10.50</td>\n",
       "      <td>20279</td>\n",
       "      <td>40</td>\n",
       "      <td>ills colour &amp; bw</td>\n",
       "      <td>15 x 21 cm</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>The Cult of Water</td>\n",
       "      <td>London</td>\n",
       "      <td>2020</td>\n",
       "      <td>9781912722723</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>colour &amp; bw</td>\n",
       "      <td>15 x 21 cm</td>\n",
       "      <td>English155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Satan is Real: Two Short Stories</td>\n",
       "      <td>London</td>\n",
       "      <td>2020</td>\n",
       "      <td>9781912722716</td>\n",
       "      <td>10.50</td>\n",
       "      <td>20280</td>\n",
       "      <td>40</td>\n",
       "      <td>Colour &amp; bw</td>\n",
       "      <td>15 x 21 cm</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Empty Aphrodite: An Encyclopaedia of Fate</td>\n",
       "      <td>Kyoto</td>\n",
       "      <td>2020</td>\n",
       "      <td>9784473044006</td>\n",
       "      <td>21.10</td>\n",
       "      <td>100</td>\n",
       "      <td>112</td>\n",
       "      <td>colour &amp; bw</td>\n",
       "      <td>13 x 19 cm</td>\n",
       "      <td>Japanese/English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>100 Beautiful Words in the Way of Tea</td>\n",
       "      <td>Kyoto</td>\n",
       "      <td>2020</td>\n",
       "      <td>9784473044006</td>\n",
       "      <td>21.10</td>\n",
       "      <td>152</td>\n",
       "      <td>200</td>\n",
       "      <td>ills colour</td>\n",
       "      <td>23 x 30 cm</td>\n",
       "      <td>Japanese/English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Errant Journal 1: Where are We?</td>\n",
       "      <td>Amsterdam</td>\n",
       "      <td>2020</td>\n",
       "      <td></td>\n",
       "      <td>38.55</td>\n",
       "      <td>20207</td>\n",
       "      <td>222</td>\n",
       "      <td>ills colour &amp; bw</td>\n",
       "      <td>17 x 21 cm</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Errant Journal, Amsterdam 2020</td>\n",
       "      <td>Amsterdam</td>\n",
       "      <td>2020</td>\n",
       "      <td>9789083079301</td>\n",
       "      <td>15.00</td>\n",
       "      <td>20307</td>\n",
       "      <td>222</td>\n",
       "      <td>ills colour &amp; bw</td>\n",
       "      <td>17 x 21 cm</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Full_title          City  Year  \\\n",
       "0                            A Magazine, Antwerp 2020       Antwerp  2020   \n",
       "1                   The Architecture of Byoungsoo Cho    Copenhagen  2020   \n",
       "2          Practice of Spiral Torch Press, Tokyo 2020         Tokyo  2020   \n",
       "3                         16. Encounters with Plečnik     La Coruña  2020   \n",
       "4   52 p, ills colour & bw, 15 x 21 cm, pb, Sloven...         Porto  2020   \n",
       "5        Pitsou Kedem Architects – Works and Projects                2020   \n",
       "6                              Robin Boyd: Late Works     Melbourne  2020   \n",
       "7             108 p, ills colour & bw, 22 x 30 cm, pb                2020   \n",
       "8   4380. Immortal: Lost Memoirs of Cornelia Dulac...      Helsinki  2020   \n",
       "9                      Monique Besten – The Wanderer*     Barcelona  2020   \n",
       "10                                           Bud Book      Helsinki  2020   \n",
       "11                      Juan Hein – Clouds and Bombs*    Copenhagen  2020   \n",
       "12  Art84. Jörg Schmeisser Retrospective: Neverend...         Tokyo  2020   \n",
       "13  Ohara Koson: Paradise On Paper Where Flowers B...         Tokyo  2019   \n",
       "14                                       butterflies.  He He, Tokyo  2020   \n",
       "15                                 Practice of Spiral         Tokyo  2020   \n",
       "16                                     79156. Goblins        London  2020   \n",
       "17                                  The Cult of Water        London  2020   \n",
       "18                   Satan is Real: Two Short Stories        London  2020   \n",
       "19          Empty Aphrodite: An Encyclopaedia of Fate         Kyoto  2020   \n",
       "20              100 Beautiful Words in the Way of Tea         Kyoto  2020   \n",
       "21                    Errant Journal 1: Where are We?     Amsterdam  2020   \n",
       "22                     Errant Journal, Amsterdam 2020     Amsterdam  2020   \n",
       "\n",
       "             ISBN  price book_shop_id pages            colour        size  \\\n",
       "0   9789077745212  15.50        20253   222       colour & bw  17 x 21 cm   \n",
       "1   9788792700322  61.70                408       colour & bw  23 x 33 cm   \n",
       "2   9784907562212  49.50        20247   304       colour & bw  17 x 24 cm   \n",
       "3   9788412162516  25.20        20203   304  ills colour & bw  17 x 24 cm   \n",
       "4   9789895462049  49.50        20161    52  ills colour & bw  15 x 21 cm   \n",
       "5   9789895462049  49.50        20161    52  ills colour & bw  15 x 21 cm   \n",
       "6   9780648435594  38.80           14   152       colour & bw  24 x 28 cm   \n",
       "7   9789462085817  39.95                108  ills colour & bw  22 x 30 cm   \n",
       "8   9789526089621  41.50        20116   128  ills colour & bw  20 x 22 cm   \n",
       "9   9788412039092  15.75                128  ills colour & bw  20 x 22 cm   \n",
       "10  9789525939262  23.20        20224   146       colour & bw  13 x 20 cm   \n",
       "11  9788797052068  30.85                182  ills colour & bw  15 x 21 cm   \n",
       "12  9784763018267  36.25        20158    44       colour & bw  20 x 25 cm   \n",
       "13  9784808711290  32.35        20313   172       ills colour  19 x 26 cm   \n",
       "14  9784908062315  83.90                144       ills colour  18 x 26 cm   \n",
       "15     4907562212  49.50        20247   200       ills colour  23 x 30 cm   \n",
       "16  9781912722730  10.50        20279    40  ills colour & bw  15 x 21 cm   \n",
       "17  9781912722723                                 colour & bw  15 x 21 cm   \n",
       "18  9781912722716  10.50        20280    40       Colour & bw  15 x 21 cm   \n",
       "19  9784473044006  21.10          100   112       colour & bw  13 x 19 cm   \n",
       "20  9784473044006  21.10          152   200       ills colour  23 x 30 cm   \n",
       "21                 38.55        20207   222  ills colour & bw  17 x 21 cm   \n",
       "22  9789083079301  15.00        20307   222  ills colour & bw  17 x 21 cm   \n",
       "\n",
       "             language  \n",
       "0             English  \n",
       "1             English  \n",
       "2     Spanish/English  \n",
       "3     Spanish/English  \n",
       "4   Slovenian/English  \n",
       "5   Slovenian/English  \n",
       "6             English  \n",
       "7             English  \n",
       "8             English  \n",
       "9             English  \n",
       "10            English  \n",
       "11            English  \n",
       "12            English  \n",
       "13           Japanese  \n",
       "14   Japanese/English  \n",
       "15   Japanese/English  \n",
       "16            English  \n",
       "17         English155  \n",
       "18            English  \n",
       "19   Japanese/English  \n",
       "20   Japanese/English  \n",
       "21            English  \n",
       "22            English  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = create_dataframe_from_json_strings(extracted_info)\n",
    "df2.head(23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import List\n",
    "from langchain.schema import Document\n",
    "from langchain.document_loaders import CSVLoader, PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def load_csv_files(directory: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load and return the content of all CSV files in the given directory.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith(\".csv\"):\n",
    "            file_path = os.path.join(directory, file_name)\n",
    "            loader = CSVLoader(file_path)\n",
    "            documents.extend(loader.load())\n",
    "    return documents\n",
    "\n",
    "def split_docs(documents: List[Document], chunk_size: int = 400, chunk_overlap: int = 40) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Split documents into chunks using RecursiveCharacterTextSplitter.\n",
    "    \n",
    "    Args:\n",
    "        documents (List[Document]): List of Document objects to be split.\n",
    "        chunk_size (int): Maximum size of each chunk.\n",
    "        chunk_overlap (int): Overlap size between chunks.\n",
    "\n",
    "    Returns:\n",
    "        List[Document]: List of split Document objects.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    return text_splitter.split_documents(documents)\n",
    "\n",
    "def remove_garbage_lines(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes lines that contain mostly numbers, standalone letters, or patterns like 'B = B', 'M = M'.\n",
    "    \"\"\"\n",
    "    cleaned_lines = []\n",
    "    \n",
    "    for line in text.split(\"\\n\"):\n",
    "        line = line.strip()\n",
    "        \n",
    "        # Skip lines that are mostly numbers, letters with =, or repeating patterns\n",
    "        if re.match(r'^([\\d\\s]+|[A-Z]\\s*=\\s*[A-Z]\\s*)+$', line):\n",
    "            continue\n",
    "        \n",
    "        # Skip lines with excessive letter-number-symbol sequences (like slurB B B 0 B B)\n",
    "        if re.search(r'(slurB|B\\s*=\\s*B|M\\s*=\\s*M|Y\\s*=\\s*Y|X\\s*=\\s*X|Z\\s*=\\s*Z)', line):\n",
    "            continue\n",
    "        \n",
    "        cleaned_lines.append(line)\n",
    "    \n",
    "    return \"\\n\".join(cleaned_lines)\n",
    "\n",
    "def load_pdf_files(directory: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load and return the content of all PDF files in the given directory.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith(\".pdf\"):\n",
    "            file_path = os.path.join(directory, file_name)\n",
    "            loader = PyPDFLoader(file_path)\n",
    "            pdf_docs = loader.load()\n",
    "            \n",
    "            for doc in pdf_docs:\n",
    "                doc.page_content = remove_garbage_lines(doc.page_content)  # Clean extracted text\n",
    "\n",
    "            documents.extend(pdf_docs)\n",
    "    return documents\n",
    "\n",
    "def upload_files(directory: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Upload all supported file types from a given directory, split PDF content into chunks, and return their content.\n",
    "    \"\"\"\n",
    "    supported_loaders = {\n",
    "        \"csv\": load_csv_files,\n",
    "        \"pdf\": load_pdf_files\n",
    "    }\n",
    "    documents = []\n",
    "\n",
    "    for ext, loader_func in supported_loaders.items():\n",
    "        loaded_documents = loader_func(directory)\n",
    "        if ext == \"pdf\":\n",
    "            documents.extend(split_docs(loaded_documents))  # Split PDFs into chunks\n",
    "        else:\n",
    "            documents.extend(loaded_documents)\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Example usage\n",
    "test_directory = 'C:/Users/skrge/Documents/GitHub/llmtesting/data/test/test'\n",
    "pdf_docs = upload_files(test_directory)\n",
    "proc_docs = process_documents(pdf_docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
